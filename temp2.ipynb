{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBart50TokenizerFast\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from utils.rlst import RLST, train_step, translate, TranslationDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                english  \\\n",
      "0      Previously on \"The Blacklist\"...   \n",
      "1        - You want to call your daddy?   \n",
      "2  - Yeah, I want to tell him I'm okay.   \n",
      "3                                 Okay.   \n",
      "4  Lizzy... Be careful of your husband.   \n",
      "\n",
      "                                   polish  \n",
      "0             /W poprzednich odcinkach: /  \n",
      "1             - Chcesz zadzwonić do taty?  \n",
      "2  - Tak, powiem, że wszystko w porządku.  \n",
      "3                                 Dobrze.  \n",
      "4                  Lizzy, uważaj na męża.  \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"pl_PL\"\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "file_reader = pd.read_csv('data/en_pl.csv', chunksize=chunk_size)\n",
    "first_chunk = next(file_reader) \n",
    "print(type(first_chunk))\n",
    "print(first_chunk.head())\n",
    "\n",
    "train_data, test_data = train_test_split(first_chunk, test_size=0.1)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "test_dataset = TranslationDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1d415fd86b4b16b41205866701a23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Nauka:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 1018.8916\n",
      "Epoch 1, Batch 20, Loss: 50.8331\n",
      "Epoch 1, Batch 40, Loss: 45.8219\n",
      "Epoch 1, Loss: 4624.7250\n",
      "Epoch 2, Batch 0, Loss: 50.5770\n",
      "Epoch 2, Batch 20, Loss: 46.1649\n",
      "Epoch 2, Batch 40, Loss: 43.3482\n",
      "Epoch 2, Loss: 2027.1185\n",
      "Epoch 3, Batch 0, Loss: 47.7526\n",
      "Epoch 3, Batch 20, Loss: 37.3129\n",
      "Epoch 3, Batch 40, Loss: 43.9011\n",
      "Epoch 3, Loss: 1825.9355\n",
      "Epoch 4, Batch 0, Loss: 39.7344\n",
      "Epoch 4, Batch 20, Loss: 31.2087\n",
      "Epoch 4, Batch 40, Loss: 29.2064\n",
      "Epoch 4, Loss: 1574.1606\n",
      "Epoch 5, Batch 0, Loss: 27.5856\n",
      "Epoch 5, Batch 20, Loss: 201.0647\n",
      "Epoch 5, Batch 40, Loss: 27.0681\n",
      "Epoch 5, Loss: 2405.8388\n",
      "Epoch 6, Batch 0, Loss: 48.3826\n",
      "Epoch 6, Batch 20, Loss: 126.9132\n",
      "Epoch 6, Batch 40, Loss: 51.0956\n",
      "Epoch 6, Loss: 2654.1612\n",
      "Epoch 7, Batch 0, Loss: 109.8427\n",
      "Epoch 7, Batch 20, Loss: 56.1180\n",
      "Epoch 7, Batch 40, Loss: 27.7556\n",
      "Epoch 7, Loss: 1886.2472\n",
      "Epoch 8, Batch 0, Loss: 28.2765\n",
      "Epoch 8, Batch 20, Loss: 53.9597\n",
      "Epoch 8, Batch 40, Loss: 18.9189\n",
      "Epoch 8, Loss: 2259.8187\n",
      "Epoch 9, Batch 0, Loss: 37.1755\n",
      "Epoch 9, Batch 20, Loss: 69.4534\n",
      "Epoch 9, Batch 40, Loss: 208.8816\n",
      "Epoch 9, Loss: 2327.5859\n",
      "Epoch 10, Batch 0, Loss: 46.0295\n",
      "Epoch 10, Batch 20, Loss: 14.0907\n",
      "Epoch 10, Batch 40, Loss: 20.9952\n",
      "Epoch 10, Loss: 2314.3230\n",
      "Translated token vectors (first sentence): at S من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من من\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/zzsn/utils/rlst.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_token = torch.tensor(input_seq[x_i], dtype=torch.float32).to(device)\n",
      "/home/filip/zzsn/utils/rlst.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_token = torch.tensor(input_seq[x_i], dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = output_dim = 256  # Placeholder: input_ids must be embedded properly\n",
    "model = RLST(input_dim, output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "loss_fn_ce = nn.MSELoss()\n",
    "loss_fn_mse = nn.MSELoss()\n",
    "epoch_len = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epoch_len), desc=\"Nauka\", position=0):\n",
    "    total_loss = 0.0\n",
    "    # for batch in tqdm(train_dataloader, desc='Batche', position=1, leave=False):\n",
    "    try:\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            x_seq, y_seq = batch\n",
    "            loss = train_step(model, optimizer, x_seq[0], y_seq[0], loss_fn_ce, loss_fn_mse, device)\n",
    "            total_loss += loss\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Inference example\n",
    "for batch in test_dataloader:\n",
    "    x_seq, _ = batch\n",
    "    prediction = translate(model, x_seq[0], device)\n",
    "    token_ids = [int(torch.argmax(torch.tensor(vec))) for vec in prediction]\n",
    "    sentence = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(\"Translated token vectors (first sentence):\", sentence)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
